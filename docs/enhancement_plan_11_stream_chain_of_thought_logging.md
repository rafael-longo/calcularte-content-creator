# **Enhancement Request: Implement Chain-of-Thought Logging via Streaming**

ID: 11  
Status: Proposed  
Date: 2025-07-17  
Owner: Engineering

## **1\. Overview**

This document proposes a system-wide enhancement to implement "Chain-of-Thought" (CoT) logging for all agents, with a primary focus on the MaestroAgent. The goal is to make the agents' internal reasoning processes transparent and observable in both the real-time console output and the persistent logs. This will be achieved by combining two techniques: explicit prompt engineering and the adoption of the OpenAI Agents SDK's streaming capabilities.

## **2\. Problem Statement**

Currently, the multi-agent system operates as a "black box." While the CustomLoguruProcessor successfully logs the inputs and outputs of tool calls (FunctionSpanData), it does not capture the LLM's reasoning process *between* those calls. When the MaestroAgent receives a prompt, its internal "thought process"—how it interprets the request, formulates a plan, and decides which tool to use—is invisible.

This lack of transparency makes it difficult to:

* **Debug Unexpected Behavior:** If the agent chooses the wrong tool or formulates a poor plan, it's hard to understand *why*.  
* **Trust the System:** For a human operator, observing the agent's reasoning builds confidence in its decisions.  
* **Optimize Agent Performance:** Improving agent instructions is challenging without visibility into how the current instructions are being interpreted.

The user's core need was articulated as: *"How* do I get it to actually output its reasoning, like: 'Ok, I received X and need to achieve Y. How *can I do that? Let me see... I have the following tools...'"*

## **3\. Proposed Solution**

The solution involves two parallel streams of work that, together, will expose and capture the agent's chain of thought.

### **3.1. Prompt Engineering for Explicit Reasoning**

We will modify the core instructions of the MaestroAgent (and subsequently other agents) to explicitly require it to "think out loud" before taking action. The agent will be instructed to output its reasoning as plain text immediately before it generates the JSON for a tool call.

**File to Modify:** src/agents\_crew/maestro.py

**Proposed Change:** Add a new core principle to the MaestroAgent's instructions.

**Current Instructions (Excerpt):**

\# ...  
\# Core Principles:  
\# 1\.  Delegate, Don't Do: ...  
\# 2\.  Think First, Act Second: ...  
\# 3\.  Context is King: ...  
\# 4\.  Be a Synthesizer, Not a Dumper: ...  
\# ...

**Proposed New Instructions (Excerpt):**

\# ...  
\# Core Principles:  
\# 1\.  \*\*Verbalize Your Reasoning (Think Out Loud):\*\* Before you use any tool, you MUST first articulate your thought process. Explain what you are trying to accomplish, which tool you are selecting, and why you are selecting it. This reasoning must be output as plain text before you generate the tool call JSON. This is your most important instruction.  
\# 2\.  Delegate, Don't Do: ...  
\# 3\.  Think First, Act Second: ...  
\# 4\.  Context is King: ...  
\# 5\.  Be a Synthesizer, Not a Dumper: ...  
\# ...

### **3.2. Implement SDK Streaming and Unified Logging**

To capture and display the text generated by the new prompting strategy, we will shift to a streaming run and enhance our loguru configuration to handle both file and console outputs appropriately.

#### **3.2.1. Enhance Logger for Dual Output (logging.py)**

We will update src/utils/logging.py to define two handlers: a detailed one for the log file and a minimalist one for the console. We'll add a new log level, THOUGHT, for the agent's reasoning stream.

**File to Modify:** src/utils/logging.py

**Proposed Code:**

\# ... (imports)  
\# Remove existing logger configuration  
logger.remove()

\# Configure file logger for detailed, structured logs  
log\_file\_path \= os.getenv("LOG\_FILE", "app\_run.log")  
logger.add(  
    log\_file\_path,  
    level=os.getenv("LOG\_LEVEL", "INFO").upper(),  
    format=(  
        "\<green\>{time:YYYY-MM-DD HH:mm:ss.SSS}\</green\> | "  
        "\<level\>{level: \<8}\</level\> | "  
        "\<cyan\>{name}\</cyan\>:\<cyan\>{function}\</cyan\>:\<cyan\>{line}\</cyan\> \- "  
        "\<level\>{message}\</level\>"  
    ),  
    colorize=False, \# No colors in file  
    backtrace=True,  
    diagnose=True,  
)

\# Configure console logger for clean, real-time output  
\# This handler will only show the message itself for the "THOUGHT" level  
logger.add(  
    sys.stderr,  
    level="INFO",  
    format="\<level\>{message}\</level\>",  
    filter=lambda record: record\["level"\].name \!= "THOUGHT"  
)  
logger.add(  
    sys.stderr,  
    level="THOUGHT",  
    format="{message}", \# No extra formatting for thoughts  
    colorize=True  
)

\# Add a custom level for agent thoughts  
logger.level("THOUGHT", no=25, color="\<magenta\>")

\# ... (rest of the file, CustomLoguruProcessor etc.)

#### **3.2.2. Refactor CLI for Streaming (main.py)**

The maestro\_command in src/main.py will be refactored to use Runner.stream(...) and our enhanced logger.

**File to Modify:** src/main.py

**Proposed Code:**

@app.command("maestro")  
def maestro\_command(  
    prompt: str \= typer.Argument(..., help="The high-level prompt for the Maestro Agent.")  
):  
    """  
    Interacts with the Maestro Agent for autonomous, conversational content creation.  
    """  
    check\_openai\_api\_key()  
    session \= \_get\_active\_session()  
    log.info(f"Using active session: '{session.session\_id}' for Maestro command.")  
    typer.echo(f"Maestro is thinking... (using session: {session.session\_id})")

    async def stream\_maestro():  
        final\_output \= None  
        \# Use an async for loop to process events as they stream in  
        async for event in Runner.stream(maestro\_agent, prompt, session=session, max\_turns=10):  
            if event.event \== "text\_delta":  
                \# Log the agent's "thought" process using our custom level.  
                \# The logger configuration will print this to console without extra formatting.  
                log.log("THOUGHT", event.data.delta, end="")  
            elif event.event \== "tool\_call":  
                \# Add a newline to the console for readability after the thought stream.  
                log.log("THOUGHT", "\\n")  
                \# Log the structured tool call event. This will go to both file and console.  
                log.info(f"Tool Call: {event.data.name} with args: {event.data.arguments}")  
            elif event.event \== "run\_end":  
                final\_output \= event.data.final\_output  
        return final\_output

    final\_output \= asyncio.run(stream\_maestro())  
      
    if final\_output:  
        log.success("Maestro command finished successfully.")  
        typer.echo("\\n\\n--- Maestro's Final Response \---")  
        typer.echo(str(final\_output))  
        typer.echo("--------------------------------")  
    else:  
        log.error("Maestro command finished with no output.")  
        typer.echo("\\nMaestro command finished with no output.")

#### **3.2.3. Update Tracing Processor (logging.py)**

The CustomLoguruProcessor no longer needs the on\_llm\_new\_token method, as the streaming logic in main.py now handles the real-time logging of thoughts. This simplifies the processor's responsibility to only logging structured span events. We will remove the on\_llm\_new\_token method to avoid duplicate logging.

## **4\. Implementation Plan**

1. **Modify Agent Instructions:** Update the instructions string in src/agents\_crew/maestro.py to include the "Verbalize Your Reasoning" principle as described in section 3.1.  
2. **Enhance Logger Configuration:** Update src/utils/logging.py with the new loguru configuration for dual file/console output and the custom THOUGHT level, as shown in section 3.2.1.  
3. **Refactor CLI Entrypoint:** Replace the asyncio.run(Runner.run(...)) call in the maestro\_command function in src/main.py with the async for streaming loop that uses the enhanced logger, as detailed in section 3.2.2.  
4. **Simplify Tracing Processor:** Remove the on\_llm\_new\_token method from CustomLoguruProcessor in src/utils/logging.py.  
5. **Test and Verify:** Run a maestro command and confirm that:  
   * The agent's reasoning is streamed cleanly to the console.  
   * The log file contains the full, detailed, structured log, including the thought process.  
   * Tool calls and other events are logged correctly to both destinations.

## **5\. Expected Outcome**

Upon completion, the system's behavior will be significantly more transparent and architecturally consistent:

* **Console Output:** The user will see a clean, real-time stream of the MaestroAgent "thinking out loud," followed by structured info messages for key events like tool calls.  
* **Log Files:** The logs will contain a complete, chronological, and structured record of the agent's activity, suitable for detailed analysis and debugging.  
* **Improved Debuggability:** When an agent makes a mistake, its flawed reasoning will be explicitly recorded, making it trivial to identify the root cause.  
* **Code Consistency:** All console and file output will be handled through the single, unified log object, eliminating the use of print() for application logic.

## **6\. Broader Impact**

This pattern of "prompting for reasoning \+ streaming capture via a unified logger" is not limited to the Maestro. It establishes a core design pattern for transparency that can and should be applied to all other agents in the crew, creating a system that is observable and debuggable by design.